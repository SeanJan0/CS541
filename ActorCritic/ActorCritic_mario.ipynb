{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing dependancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.spaces import Box\n",
    "from gym.wrappers import FrameStack\n",
    "import gym_super_mario_bros\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition\n",
    "\n",
    "We can use the state variables (position of cart, angle of pole)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.conv1 = nn.LazyConv2d(8, 5)\n",
    "        self.conv2 = nn.LazyConv2d(16, 5)\n",
    "        self.conv3 = nn.LazyConv2d(16, 7)\n",
    "        self.bn1 = nn.LazyBatchNorm2d()\n",
    "        self.bn2 = nn.LazyBatchNorm2d()\n",
    "        self.bn3 = nn.LazyBatchNorm2d()\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        self.fn1 = nn.LazyLinear(32)\n",
    "        self.a1 = nn.LazyLinear(8) \n",
    "        self.a2 = nn.LazyLinear(output_size) \n",
    "        self.c1 = nn.LazyLinear(8) \n",
    "        self.c2 = nn.LazyLinear(1) \n",
    "\n",
    "        self.act = nn.PReLU()\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.act(self.bn1(self.conv1(input)))\n",
    "        x = self.act(self.bn2(self.conv2(x)))\n",
    "        x = self.act(self.bn3(self.conv3(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = self.act(self.fn1(x))\n",
    "        a_output = self.act(self.a1(x))\n",
    "        a_output = self.softmax(self.a2(x))\n",
    "        c_output = self.act(self.c1(x))\n",
    "        c_output = self.c2(x)\n",
    "        return a_output, c_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticAgent:\n",
    "    def __init__(self, env):\n",
    "\n",
    "        self.action_size = env.action_space.n # output size\n",
    "        \n",
    "        self.model = ActorCritic(self.action_size).to(device)\n",
    "\n",
    "        self.critic_loss = nn.SmoothL1Loss()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.0001)\n",
    "\n",
    "        self.num_epochs = 500\n",
    "        self.gamma = 0.99\n",
    "\n",
    "    def act(self, state):\n",
    "        action_prob, critique = self.model(state)\n",
    "        m = Categorical(action_prob)\n",
    "        action = m.sample()\n",
    "        return action.item(), critique, m.log_prob(action)\n",
    "    \n",
    "    def act_ideal(self, state):\n",
    "        actions, critique = self.model(state)\n",
    "        return actions.max(1)[1].item()\n",
    "    \n",
    "    def train_step(self, log_probs, critiques, rewards):\n",
    "        \n",
    "        discount_reward_sum = 0\n",
    "        returns = []\n",
    "\n",
    "        for reward in rewards[::-1]:\n",
    "            discount_reward_sum = reward + self.gamma * discount_reward_sum\n",
    "            returns.insert(0, discount_reward_sum)\n",
    "\n",
    "        returns = np.array(returns)\n",
    "        returns = (returns - np.mean(returns)) / (np.std(returns) + np.finfo(np.float32).eps.item())\n",
    "        returns = returns.tolist()\n",
    "\n",
    "        policy_losses = []\n",
    "        critic_losses = []\n",
    "        for log_prob, critique, r in zip(log_probs, critiques, returns):\n",
    "            advantage = r - critique\n",
    "\n",
    "            policy_losses.append(-log_prob * advantage)\n",
    "            critic_losses.append(self.critic_loss(critique, torch.Tensor([r]).to(device)))\n",
    "        \n",
    "        loss = torch.stack(policy_losses).sum() + torch.stack(critic_losses).sum()\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    \n",
    "    def train(self, env, print_epochs=False, render=False, checkpoint=True):\n",
    "\n",
    "        scores = []\n",
    "\n",
    "        for epoch in range(self.num_epochs):\n",
    "            done = False\n",
    "            curr_state = env.reset()\n",
    "            curr_state = torch.Tensor(np.array(curr_state)).to(device)\n",
    "            score = 0\n",
    "\n",
    "            log_probs = []\n",
    "            critiques = []\n",
    "            rewards = []\n",
    "            \n",
    "            while not done:\n",
    "                if render:\n",
    "                    env.render()\n",
    "\n",
    "                action, critique, log_prob = self.act(curr_state.unsqueeze(0))\n",
    "                curr_state, reward, done, _ = env.step(action)\n",
    "                curr_state = torch.Tensor(np.array(curr_state)).to(device)\n",
    "\n",
    "                log_probs.append(log_prob)\n",
    "                critiques.append(critique)\n",
    "                rewards.append(reward)\n",
    "\n",
    "                score += reward\n",
    "\n",
    "            self.train_step(log_probs, critiques, rewards)\n",
    "\n",
    "            scores.append(score)\n",
    "\n",
    "            if print_epochs:\n",
    "                print(\"Epoch: \" + str(epoch + 1) + \". Score is: \" + str(score))\n",
    "            \n",
    "            if checkpoint and epoch % 50 == 0:\n",
    "                torch.save(self, 'mario_ac.pt')\n",
    "                np.save('mario_ac_scores.npy', scores)\n",
    "\n",
    "        torch.save(self, 'mario_ac.pt')\n",
    "        np.save('mario_ac_scores.npy', scores)\n",
    "\n",
    "            \n",
    "        return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, and sum reward\"\"\"\n",
    "        total_reward = 0.0\n",
    "        done = False\n",
    "        for i in range(self._skip):\n",
    "            # Accumulate reward and repeat the same action\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, info\n",
    "\n",
    "\n",
    "class GrayScaleObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        obs_shape = self.observation_space.shape[:2]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def permute_orientation(self, observation):\n",
    "        # permute [H, W, C] array to [C, H, W] tensor\n",
    "        observation = np.transpose(observation, (2, 0, 1))\n",
    "        observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
    "        return observation\n",
    "\n",
    "    def observation(self, observation):\n",
    "        observation = self.permute_orientation(observation)\n",
    "        transform = T.Grayscale()\n",
    "        observation = transform(observation)\n",
    "        return observation\n",
    "\n",
    "\n",
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super().__init__(env)\n",
    "        if isinstance(shape, int):\n",
    "            self.shape = (shape, shape)\n",
    "        else:\n",
    "            self.shape = tuple(shape)\n",
    "\n",
    "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        transforms = T.Compose(\n",
    "            [T.Resize(self.shape), T.Normalize(0, 255)]\n",
    "        )\n",
    "        observation = transforms(observation).squeeze(0)\n",
    "        return observation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sean Jan's PC\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n",
      "C:\\Users\\Sean Jan's PC\\AppData\\Local\\Temp\\ipykernel_22536\\3179329841.py:29: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  a_output = self.softmax(self.a2(x))\n",
      "C:\\Users\\Sean Jan's PC\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\loss.py:912: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1. Score is: 969.0\n",
      "Epoch: 2. Score is: 609.0\n",
      "Epoch: 3. Score is: 619.0\n",
      "Epoch: 4. Score is: 1022.0\n",
      "Epoch: 5. Score is: 232.0\n",
      "Epoch: 6. Score is: 1046.0\n",
      "Epoch: 7. Score is: 1046.0\n",
      "Epoch: 8. Score is: 232.0\n",
      "Epoch: 9. Score is: 1389.0\n",
      "Epoch: 10. Score is: 232.0\n",
      "Epoch: 11. Score is: 232.0\n",
      "Epoch: 12. Score is: 630.0\n",
      "Epoch: 13. Score is: 800.0\n",
      "Epoch: 14. Score is: 1298.0\n",
      "Epoch: 15. Score is: 1025.0\n",
      "Epoch: 16. Score is: 604.0\n",
      "Epoch: 17. Score is: 232.0\n",
      "Epoch: 18. Score is: 1310.0\n",
      "Epoch: 19. Score is: 1294.0\n",
      "Epoch: 20. Score is: 232.0\n",
      "Epoch: 21. Score is: 1028.0\n",
      "Epoch: 22. Score is: 636.0\n",
      "Epoch: 23. Score is: 1029.0\n",
      "Epoch: 24. Score is: 809.0\n",
      "Epoch: 25. Score is: 810.0\n",
      "Epoch: 26. Score is: 803.0\n",
      "Epoch: 27. Score is: 1048.0\n",
      "Epoch: 28. Score is: 1299.0\n",
      "Epoch: 29. Score is: 808.0\n",
      "Epoch: 30. Score is: 1298.0\n",
      "Epoch: 31. Score is: 593.0\n",
      "Epoch: 32. Score is: 1276.0\n",
      "Epoch: 33. Score is: 598.0\n",
      "Epoch: 34. Score is: 1011.0\n",
      "Epoch: 35. Score is: 1132.0\n",
      "Epoch: 36. Score is: 1151.0\n",
      "Epoch: 37. Score is: 1312.0\n",
      "Epoch: 38. Score is: 600.0\n",
      "Epoch: 39. Score is: 1300.0\n",
      "Epoch: 40. Score is: 1008.0\n",
      "Epoch: 41. Score is: 1312.0\n",
      "Epoch: 42. Score is: 750.0\n",
      "Epoch: 43. Score is: 1033.0\n",
      "Epoch: 44. Score is: 1014.0\n",
      "Epoch: 45. Score is: 1667.0\n",
      "Epoch: 46. Score is: 1399.0\n",
      "Epoch: 47. Score is: 232.0\n",
      "Epoch: 48. Score is: 991.0\n",
      "Epoch: 49. Score is: 216.0\n",
      "Epoch: 50. Score is: 735.0\n",
      "Epoch: 51. Score is: 224.0\n",
      "Epoch: 52. Score is: 631.0\n",
      "Epoch: 53. Score is: 585.0\n",
      "Epoch: 54. Score is: 735.0\n",
      "Epoch: 55. Score is: 1308.0\n",
      "Epoch: 56. Score is: 1045.0\n",
      "Epoch: 57. Score is: 732.0\n",
      "Epoch: 58. Score is: 804.0\n",
      "Epoch: 59. Score is: 1304.0\n",
      "Epoch: 60. Score is: 1034.0\n",
      "Epoch: 61. Score is: 712.0\n",
      "Epoch: 62. Score is: 1298.0\n",
      "Epoch: 63. Score is: 1026.0\n",
      "Epoch: 64. Score is: 770.0\n",
      "Epoch: 65. Score is: 1014.0\n",
      "Epoch: 66. Score is: 232.0\n",
      "Epoch: 67. Score is: 1033.0\n",
      "Epoch: 68. Score is: 600.0\n",
      "Epoch: 69. Score is: 232.0\n",
      "Epoch: 70. Score is: 582.0\n",
      "Epoch: 71. Score is: 223.0\n",
      "Epoch: 72. Score is: 704.0\n",
      "Epoch: 73. Score is: 999.0\n",
      "Epoch: 74. Score is: 223.0\n",
      "Epoch: 75. Score is: 232.0\n",
      "Epoch: 76. Score is: 1045.0\n",
      "Epoch: 77. Score is: 223.0\n",
      "Epoch: 78. Score is: 1387.0\n",
      "Epoch: 79. Score is: 589.0\n",
      "Epoch: 80. Score is: 217.0\n",
      "Epoch: 81. Score is: 1040.0\n",
      "Epoch: 82. Score is: 232.0\n",
      "Epoch: 83. Score is: 225.0\n",
      "Epoch: 84. Score is: 1674.0\n",
      "Epoch: 85. Score is: 216.0\n",
      "Epoch: 86. Score is: 1043.0\n",
      "Epoch: 87. Score is: 1040.0\n",
      "Epoch: 88. Score is: 1023.0\n",
      "Epoch: 89. Score is: 1306.0\n",
      "Epoch: 90. Score is: 232.0\n",
      "Epoch: 91. Score is: 232.0\n",
      "Epoch: 92. Score is: 619.0\n",
      "Epoch: 93. Score is: 1283.0\n",
      "Epoch: 94. Score is: 232.0\n",
      "Epoch: 95. Score is: 634.0\n",
      "Epoch: 96. Score is: 1040.0\n",
      "Epoch: 97. Score is: 1039.0\n",
      "Epoch: 98. Score is: 1116.0\n",
      "Epoch: 99. Score is: 1278.0\n",
      "Epoch: 100. Score is: 709.0\n",
      "Epoch: 101. Score is: 225.0\n",
      "Epoch: 102. Score is: 1669.0\n",
      "Epoch: 103. Score is: 734.0\n",
      "Epoch: 104. Score is: 786.0\n",
      "Epoch: 105. Score is: 1888.0\n",
      "Epoch: 106. Score is: 619.0\n",
      "Epoch: 107. Score is: 774.0\n",
      "Epoch: 108. Score is: 712.0\n",
      "Epoch: 109. Score is: 232.0\n",
      "Epoch: 110. Score is: 1298.0\n",
      "Epoch: 111. Score is: 967.0\n",
      "Epoch: 112. Score is: 596.0\n",
      "Epoch: 113. Score is: 1013.0\n",
      "Epoch: 114. Score is: 633.0\n",
      "Epoch: 115. Score is: 1122.0\n",
      "Epoch: 116. Score is: 619.0\n",
      "Epoch: 117. Score is: 1253.0\n",
      "Epoch: 118. Score is: 597.0\n",
      "Epoch: 119. Score is: 611.0\n",
      "Epoch: 120. Score is: 745.0\n",
      "Epoch: 121. Score is: 232.0\n",
      "Epoch: 122. Score is: 588.0\n",
      "Epoch: 123. Score is: 589.0\n",
      "Epoch: 124. Score is: 1004.0\n",
      "Epoch: 125. Score is: 1249.0\n",
      "Epoch: 126. Score is: 588.0\n",
      "Epoch: 127. Score is: 597.0\n",
      "Epoch: 128. Score is: 1277.0\n",
      "Epoch: 129. Score is: 1039.0\n",
      "Epoch: 130. Score is: 232.0\n",
      "Epoch: 131. Score is: 232.0\n",
      "Epoch: 132. Score is: 232.0\n",
      "Epoch: 133. Score is: 988.0\n",
      "Epoch: 134. Score is: 594.0\n",
      "Epoch: 135. Score is: 1272.0\n",
      "Epoch: 136. Score is: 601.0\n",
      "Epoch: 137. Score is: 602.0\n",
      "Epoch: 138. Score is: 1025.0\n",
      "Epoch: 139. Score is: 731.0\n",
      "Epoch: 140. Score is: 589.0\n",
      "Epoch: 141. Score is: 232.0\n",
      "Epoch: 142. Score is: 718.0\n",
      "Epoch: 143. Score is: 632.0\n",
      "Epoch: 144. Score is: 713.0\n",
      "Epoch: 145. Score is: 1026.0\n",
      "Epoch: 146. Score is: 634.0\n",
      "Epoch: 147. Score is: 618.0\n",
      "Epoch: 148. Score is: 1165.0\n",
      "Epoch: 149. Score is: 1290.0\n",
      "Epoch: 150. Score is: 1293.0\n",
      "Epoch: 151. Score is: 718.0\n",
      "Epoch: 152. Score is: 591.0\n",
      "Epoch: 153. Score is: 1403.0\n",
      "Epoch: 154. Score is: 634.0\n",
      "Epoch: 155. Score is: 603.0\n",
      "Epoch: 156. Score is: 743.0\n",
      "Epoch: 157. Score is: 764.0\n",
      "Epoch: 158. Score is: 1007.0\n",
      "Epoch: 159. Score is: 1036.0\n",
      "Epoch: 160. Score is: 632.0\n",
      "Epoch: 161. Score is: 634.0\n",
      "Epoch: 162. Score is: 807.0\n",
      "Epoch: 163. Score is: 1302.0\n",
      "Epoch: 164. Score is: 634.0\n",
      "Epoch: 165. Score is: 1040.0\n",
      "Epoch: 166. Score is: 1306.0\n",
      "Epoch: 167. Score is: 216.0\n",
      "Epoch: 168. Score is: 591.0\n",
      "Epoch: 169. Score is: 222.0\n",
      "Epoch: 170. Score is: 606.0\n",
      "Epoch: 171. Score is: 1030.0\n",
      "Epoch: 172. Score is: 232.0\n",
      "Epoch: 173. Score is: 714.0\n",
      "Epoch: 174. Score is: 723.0\n",
      "Epoch: 175. Score is: 1125.0\n",
      "Epoch: 176. Score is: 2995.0\n",
      "Epoch: 177. Score is: 1038.0\n",
      "Epoch: 178. Score is: 745.0\n",
      "Epoch: 179. Score is: 232.0\n",
      "Epoch: 180. Score is: 594.0\n",
      "Epoch: 181. Score is: 619.0\n",
      "Epoch: 182. Score is: 1044.0\n",
      "Epoch: 183. Score is: 1046.0\n",
      "Epoch: 184. Score is: 1311.0\n",
      "Epoch: 185. Score is: 216.0\n",
      "Epoch: 186. Score is: 1042.0\n",
      "Epoch: 187. Score is: 1395.0\n",
      "Epoch: 188. Score is: 1413.0\n",
      "Epoch: 189. Score is: 1034.0\n",
      "Epoch: 190. Score is: 778.0\n",
      "Epoch: 191. Score is: 1030.0\n",
      "Epoch: 192. Score is: 1025.0\n",
      "Epoch: 193. Score is: 632.0\n",
      "Epoch: 194. Score is: 216.0\n",
      "Epoch: 195. Score is: 1043.0\n",
      "Epoch: 196. Score is: 772.0\n",
      "Epoch: 197. Score is: 740.0\n",
      "Epoch: 198. Score is: 1300.0\n",
      "Epoch: 199. Score is: 232.0\n",
      "Epoch: 200. Score is: 606.0\n",
      "Epoch: 201. Score is: 232.0\n",
      "Epoch: 202. Score is: 628.0\n",
      "Epoch: 203. Score is: 636.0\n",
      "Epoch: 204. Score is: 588.0\n",
      "Epoch: 205. Score is: 1042.0\n",
      "Epoch: 206. Score is: 1046.0\n",
      "Epoch: 207. Score is: 713.0\n",
      "Epoch: 208. Score is: 1302.0\n",
      "Epoch: 209. Score is: 1034.0\n",
      "Epoch: 210. Score is: 1037.0\n",
      "Epoch: 211. Score is: 1049.0\n",
      "Epoch: 212. Score is: 1309.0\n",
      "Epoch: 213. Score is: 216.0\n",
      "Epoch: 214. Score is: 715.0\n",
      "Epoch: 215. Score is: 1031.0\n",
      "Epoch: 216. Score is: 1041.0\n",
      "Epoch: 217. Score is: 803.0\n",
      "Epoch: 218. Score is: 632.0\n",
      "Epoch: 219. Score is: 1397.0\n",
      "Epoch: 220. Score is: 616.0\n",
      "Epoch: 221. Score is: 2986.0\n",
      "Epoch: 222. Score is: 1319.0\n",
      "Epoch: 223. Score is: 632.0\n",
      "Epoch: 224. Score is: 717.0\n",
      "Epoch: 225. Score is: 604.0\n",
      "Epoch: 226. Score is: 1041.0\n",
      "Epoch: 227. Score is: 806.0\n",
      "Epoch: 228. Score is: 621.0\n",
      "Epoch: 229. Score is: 1302.0\n",
      "Epoch: 230. Score is: 1020.0\n",
      "Epoch: 231. Score is: 1037.0\n",
      "Epoch: 232. Score is: 1043.0\n",
      "Epoch: 233. Score is: 591.0\n",
      "Epoch: 234. Score is: 1311.0\n",
      "Epoch: 235. Score is: 1045.0\n",
      "Epoch: 236. Score is: 1042.0\n",
      "Epoch: 237. Score is: 767.0\n",
      "Epoch: 238. Score is: 589.0\n",
      "Epoch: 239. Score is: 1044.0\n",
      "Epoch: 240. Score is: 1048.0\n",
      "Epoch: 241. Score is: 1047.0\n",
      "Epoch: 242. Score is: 711.0\n",
      "Epoch: 243. Score is: 1041.0\n",
      "Epoch: 244. Score is: 1031.0\n",
      "Epoch: 245. Score is: 607.0\n",
      "Epoch: 246. Score is: 591.0\n",
      "Epoch: 247. Score is: 1043.0\n",
      "Epoch: 248. Score is: 1035.0\n",
      "Epoch: 249. Score is: 589.0\n",
      "Epoch: 250. Score is: 1045.0\n",
      "Epoch: 251. Score is: 216.0\n",
      "Epoch: 252. Score is: 1306.0\n",
      "Epoch: 253. Score is: 636.0\n",
      "Epoch: 254. Score is: 216.0\n",
      "Epoch: 255. Score is: 1315.0\n",
      "Epoch: 256. Score is: 1390.0\n",
      "Epoch: 257. Score is: 1029.0\n",
      "Epoch: 258. Score is: 699.0\n",
      "Epoch: 259. Score is: 610.0\n",
      "Epoch: 260. Score is: 1040.0\n",
      "Epoch: 261. Score is: 223.0\n",
      "Epoch: 262. Score is: 604.0\n",
      "Epoch: 263. Score is: 805.0\n",
      "Epoch: 264. Score is: 578.0\n",
      "Epoch: 265. Score is: 232.0\n",
      "Epoch: 266. Score is: 615.0\n",
      "Epoch: 267. Score is: 632.0\n",
      "Epoch: 268. Score is: 1402.0\n",
      "Epoch: 269. Score is: 232.0\n",
      "Epoch: 270. Score is: 1290.0\n",
      "Epoch: 271. Score is: 622.0\n",
      "Epoch: 272. Score is: 232.0\n",
      "Epoch: 273. Score is: 232.0\n",
      "Epoch: 274. Score is: 232.0\n",
      "Epoch: 275. Score is: 1310.0\n",
      "Epoch: 276. Score is: 1024.0\n",
      "Epoch: 277. Score is: 1045.0\n",
      "Epoch: 278. Score is: 1322.0\n",
      "Epoch: 279. Score is: 608.0\n",
      "Epoch: 280. Score is: 225.0\n",
      "Epoch: 281. Score is: 1046.0\n",
      "Epoch: 282. Score is: 600.0\n",
      "Epoch: 283. Score is: 721.0\n",
      "Epoch: 284. Score is: 601.0\n",
      "Epoch: 285. Score is: 604.0\n",
      "Epoch: 286. Score is: 621.0\n",
      "Epoch: 287. Score is: 1294.0\n",
      "Epoch: 288. Score is: 1541.0\n",
      "Epoch: 289. Score is: 704.0\n",
      "Epoch: 290. Score is: 600.0\n",
      "Epoch: 291. Score is: 1044.0\n",
      "Epoch: 292. Score is: 593.0\n",
      "Epoch: 293. Score is: 216.0\n",
      "Epoch: 294. Score is: 1323.0\n",
      "Epoch: 295. Score is: 1310.0\n",
      "Epoch: 296. Score is: 1312.0\n",
      "Epoch: 297. Score is: 632.0\n",
      "Epoch: 298. Score is: 598.0\n",
      "Epoch: 299. Score is: 746.0\n",
      "Epoch: 300. Score is: 706.0\n",
      "Epoch: 301. Score is: 1319.0\n",
      "Epoch: 302. Score is: 634.0\n",
      "Epoch: 303. Score is: 1038.0\n",
      "Epoch: 304. Score is: 1032.0\n",
      "Epoch: 305. Score is: 1036.0\n",
      "Epoch: 306. Score is: 715.0\n",
      "Epoch: 307. Score is: 762.0\n",
      "Epoch: 308. Score is: 635.0\n",
      "Epoch: 309. Score is: 1040.0\n",
      "Epoch: 310. Score is: 632.0\n",
      "Epoch: 311. Score is: 746.0\n",
      "Epoch: 312. Score is: 1041.0\n",
      "Epoch: 313. Score is: 1310.0\n",
      "Epoch: 314. Score is: 225.0\n",
      "Epoch: 315. Score is: 1019.0\n",
      "Epoch: 316. Score is: 1028.0\n",
      "Epoch: 317. Score is: 222.0\n",
      "Epoch: 318. Score is: 1020.0\n",
      "Epoch: 319. Score is: 1310.0\n",
      "Epoch: 320. Score is: 634.0\n",
      "Epoch: 321. Score is: 1016.0\n",
      "Epoch: 322. Score is: 1029.0\n",
      "Epoch: 323. Score is: 1301.0\n",
      "Epoch: 324. Score is: 619.0\n",
      "Epoch: 325. Score is: 619.0\n",
      "Epoch: 326. Score is: 1035.0\n",
      "Epoch: 327. Score is: 1306.0\n",
      "Epoch: 328. Score is: 636.0\n",
      "Epoch: 329. Score is: 1127.0\n",
      "Epoch: 330. Score is: 1016.0\n",
      "Epoch: 331. Score is: 606.0\n",
      "Epoch: 332. Score is: 1304.0\n",
      "Epoch: 333. Score is: 1023.0\n",
      "Epoch: 334. Score is: 1039.0\n",
      "Epoch: 335. Score is: 1308.0\n",
      "Epoch: 336. Score is: 738.0\n",
      "Epoch: 337. Score is: 1289.0\n",
      "Epoch: 338. Score is: 809.0\n",
      "Epoch: 339. Score is: 618.0\n",
      "Epoch: 340. Score is: 760.0\n",
      "Epoch: 341. Score is: 777.0\n",
      "Epoch: 342. Score is: 1882.0\n",
      "Epoch: 343. Score is: 1815.0\n",
      "Epoch: 344. Score is: 1045.0\n",
      "Epoch: 345. Score is: 1029.0\n",
      "Epoch: 346. Score is: 1025.0\n",
      "Epoch: 347. Score is: 1542.0\n",
      "Epoch: 348. Score is: 1403.0\n",
      "Epoch: 349. Score is: 1302.0\n",
      "Epoch: 350. Score is: 1036.0\n",
      "Epoch: 351. Score is: 1315.0\n",
      "Epoch: 352. Score is: 232.0\n",
      "Epoch: 353. Score is: 608.0\n",
      "Epoch: 354. Score is: 232.0\n",
      "Epoch: 355. Score is: 225.0\n",
      "Epoch: 356. Score is: 633.0\n",
      "Epoch: 357. Score is: 1402.0\n",
      "Epoch: 358. Score is: 1046.0\n",
      "Epoch: 359. Score is: 720.0\n",
      "Epoch: 360. Score is: 790.0\n",
      "Epoch: 361. Score is: 232.0\n",
      "Epoch: 362. Score is: 1041.0\n",
      "Epoch: 363. Score is: 727.0\n",
      "Epoch: 364. Score is: 2325.0\n",
      "Epoch: 365. Score is: 713.0\n",
      "Epoch: 366. Score is: 593.0\n",
      "Epoch: 367. Score is: 1046.0\n",
      "Epoch: 368. Score is: 1305.0\n",
      "Epoch: 369. Score is: 232.0\n",
      "Epoch: 370. Score is: 1041.0\n",
      "Epoch: 371. Score is: 1031.0\n",
      "Epoch: 372. Score is: 1048.0\n",
      "Epoch: 373. Score is: 1045.0\n",
      "Epoch: 374. Score is: 805.0\n",
      "Epoch: 375. Score is: 232.0\n",
      "Epoch: 376. Score is: 746.0\n",
      "Epoch: 377. Score is: 1037.0\n",
      "Epoch: 378. Score is: 1528.0\n",
      "Epoch: 379. Score is: 1300.0\n",
      "Epoch: 380. Score is: 1037.0\n",
      "Epoch: 381. Score is: 1305.0\n",
      "Epoch: 382. Score is: 1665.0\n",
      "Epoch: 383. Score is: 1128.0\n",
      "Epoch: 384. Score is: 1305.0\n",
      "Epoch: 385. Score is: 619.0\n",
      "Epoch: 386. Score is: 1319.0\n",
      "Epoch: 387. Score is: 1050.0\n",
      "Epoch: 388. Score is: 593.0\n",
      "Epoch: 389. Score is: 1319.0\n",
      "Epoch: 390. Score is: 232.0\n",
      "Epoch: 391. Score is: 1049.0\n",
      "Epoch: 392. Score is: 1888.0\n",
      "Epoch: 393. Score is: 1044.0\n",
      "Epoch: 394. Score is: 1032.0\n",
      "Epoch: 395. Score is: 1046.0\n",
      "Epoch: 396. Score is: 636.0\n",
      "Epoch: 397. Score is: 1301.0\n",
      "Epoch: 398. Score is: 598.0\n",
      "Epoch: 399. Score is: 622.0\n",
      "Epoch: 400. Score is: 630.0\n",
      "Epoch: 401. Score is: 1512.0\n",
      "Epoch: 402. Score is: 759.0\n",
      "Epoch: 403. Score is: 1024.0\n",
      "Epoch: 404. Score is: 610.0\n",
      "Epoch: 405. Score is: 617.0\n",
      "Epoch: 406. Score is: 1041.0\n",
      "Epoch: 407. Score is: 1030.0\n",
      "Epoch: 408. Score is: 1321.0\n",
      "Epoch: 409. Score is: 621.0\n",
      "Epoch: 410. Score is: 780.0\n",
      "Epoch: 411. Score is: 1040.0\n",
      "Epoch: 412. Score is: 1045.0\n",
      "Epoch: 413. Score is: 622.0\n",
      "Epoch: 414. Score is: 1041.0\n",
      "Epoch: 415. Score is: 802.0\n",
      "Epoch: 416. Score is: 1319.0\n",
      "Epoch: 417. Score is: 763.0\n",
      "Epoch: 418. Score is: 1314.0\n",
      "Epoch: 419. Score is: 1035.0\n",
      "Epoch: 420. Score is: 732.0\n",
      "Epoch: 421. Score is: 1314.0\n",
      "Epoch: 422. Score is: 732.0\n",
      "Epoch: 423. Score is: 792.0\n",
      "Epoch: 424. Score is: 1027.0\n",
      "Epoch: 425. Score is: 1042.0\n",
      "Epoch: 426. Score is: 707.0\n",
      "Epoch: 427. Score is: 772.0\n",
      "Epoch: 428. Score is: 755.0\n",
      "Epoch: 429. Score is: 1293.0\n",
      "Epoch: 430. Score is: 716.0\n",
      "Epoch: 431. Score is: 588.0\n",
      "Epoch: 432. Score is: 708.0\n",
      "Epoch: 433. Score is: 1015.0\n",
      "Epoch: 434. Score is: 767.0\n",
      "Epoch: 435. Score is: 636.0\n",
      "Epoch: 436. Score is: 1034.0\n",
      "Epoch: 437. Score is: 750.0\n",
      "Epoch: 438. Score is: 773.0\n",
      "Epoch: 439. Score is: 1020.0\n",
      "Epoch: 440. Score is: 232.0\n",
      "Epoch: 441. Score is: 612.0\n",
      "Epoch: 442. Score is: 1292.0\n",
      "Epoch: 443. Score is: 759.0\n",
      "Epoch: 444. Score is: 599.0\n",
      "Epoch: 445. Score is: 604.0\n",
      "Epoch: 446. Score is: 593.0\n",
      "Epoch: 447. Score is: 610.0\n",
      "Epoch: 448. Score is: 636.0\n",
      "Epoch: 449. Score is: 782.0\n",
      "Epoch: 450. Score is: 232.0\n",
      "Epoch: 451. Score is: 603.0\n",
      "Epoch: 452. Score is: 232.0\n",
      "Epoch: 453. Score is: 601.0\n",
      "Epoch: 454. Score is: 605.0\n",
      "Epoch: 455. Score is: 715.0\n",
      "Epoch: 456. Score is: 603.0\n",
      "Epoch: 457. Score is: 735.0\n",
      "Epoch: 458. Score is: 595.0\n",
      "Epoch: 459. Score is: 225.0\n",
      "Epoch: 460. Score is: 608.0\n",
      "Epoch: 461. Score is: 1033.0\n",
      "Epoch: 462. Score is: 807.0\n",
      "Epoch: 463. Score is: 609.0\n",
      "Epoch: 464. Score is: 232.0\n",
      "Epoch: 465. Score is: 587.0\n",
      "Epoch: 466. Score is: 1406.0\n",
      "Epoch: 467. Score is: 603.0\n",
      "Epoch: 468. Score is: 602.0\n",
      "Epoch: 469. Score is: 596.0\n",
      "Epoch: 470. Score is: 1234.0\n",
      "Epoch: 471. Score is: 1388.0\n",
      "Epoch: 472. Score is: 225.0\n",
      "Epoch: 473. Score is: 1308.0\n",
      "Epoch: 474. Score is: 1423.0\n",
      "Epoch: 475. Score is: 617.0\n",
      "Epoch: 476. Score is: 603.0\n",
      "Epoch: 477. Score is: 606.0\n",
      "Epoch: 478. Score is: 623.0\n",
      "Epoch: 479. Score is: 1310.0\n",
      "Epoch: 480. Score is: 1417.0\n",
      "Epoch: 481. Score is: 600.0\n",
      "Epoch: 482. Score is: 1400.0\n",
      "Epoch: 483. Score is: 1029.0\n",
      "Epoch: 484. Score is: 586.0\n",
      "Epoch: 485. Score is: 603.0\n",
      "Epoch: 486. Score is: 1040.0\n",
      "Epoch: 487. Score is: 634.0\n",
      "Epoch: 488. Score is: 600.0\n",
      "Epoch: 489. Score is: 1324.0\n",
      "Epoch: 490. Score is: 1035.0\n",
      "Epoch: 491. Score is: 619.0\n",
      "Epoch: 492. Score is: 582.0\n",
      "Epoch: 493. Score is: 748.0\n",
      "Epoch: 494. Score is: 1034.0\n",
      "Epoch: 495. Score is: 1406.0\n",
      "Epoch: 496. Score is: 606.0\n",
      "Epoch: 497. Score is: 225.0\n",
      "Epoch: 498. Score is: 1042.0\n",
      "Epoch: 499. Score is: 2305.0\n",
      "Epoch: 500. Score is: 1411.0\n"
     ]
    }
   ],
   "source": [
    "env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v3\")\n",
    "\n",
    "env = JoypadSpace(env, [[\"right\"], [\"right\", \"A\"]])\n",
    "env = SkipFrame(env, skip=4)\n",
    "env = GrayScaleObservation(env)\n",
    "env = ResizeObservation(env, shape=64)\n",
    "env = FrameStack(env, num_stack=4)\n",
    "\n",
    "agent = ActorCriticAgent(env)\n",
    "scores = agent.train(env, print_epochs=True, checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24196\\2347517875.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(scores)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c0f8c7c35006bc217a7b718b84792749372e799ff4ec2664fb84cfa3b37974dd"
  },
  "kernelspec": {
   "display_name": "Python 3.8.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
