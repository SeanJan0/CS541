{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing dependancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition\n",
    "\n",
    "We can use the state variables (position of cart, angle of pole)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fn1 = nn.Linear(input_size, 64)\n",
    "        self.fn2 = nn.Linear(64, 64)\n",
    "        self.fn3 = nn.Linear(64, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fn1(x))\n",
    "        x = F.relu(self.fn2(x))\n",
    "        x = F.relu(self.fn3(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory\n",
    "The agent should memorize what it learned before so it can learn from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = namedtuple('Experience', ('curr_state', 'action', 'next_state', 'reward', 'is_done'))\n",
    "\n",
    "class Memory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def remember(self, *args):\n",
    "        self.memory.append(Experience(*args))\n",
    "\n",
    "    def recall(self, batch_size):\n",
    "        experiences = random.sample(self.memory, batch_size)\n",
    "        batch = Experience(*zip(*experiences))\n",
    "        return batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, env):\n",
    "\n",
    "        self.state_size = env.observation_space.shape[0] # input size\n",
    "        self.action_size = env.action_space.n # output size\n",
    "        \n",
    "        self.model = DQN(self.state_size, self.action_size).to(device)\n",
    "\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
    "\n",
    "        self.exploration_rate = 1 # initial exploration rate, always leave at 1\n",
    "        self.exploration_rate_decay = 0.9999 # rate at which the exploration decreases\n",
    "        self.exploration_rate_min = 0.1 # minimun exploration rate\n",
    "        \n",
    "        self.gamma = 0.99 # falloff for Q score\n",
    "\n",
    "        self.batch_size = 64\n",
    "        self.num_epochs = 200\n",
    "\n",
    "        self.memory = Memory(10000) # how many of the previous samples are used\n",
    "    \n",
    "    def act(self, state):\n",
    "        if random.random() < self.exploration_rate:\n",
    "            action = random.randrange(self.action_size) # act randomly\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                action = self.act_ideal(state) # act ideally\n",
    "            \n",
    "        self.exploration_rate *= self.exploration_rate_decay\n",
    "        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def act_ideal(self, state):\n",
    "        return self.model(state).max(1)[1].item()\n",
    "    \n",
    "    def train_step(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        batch = self.memory.recall(self.batch_size)\n",
    "        \n",
    "        curr_states = torch.stack(batch.curr_state).squeeze(1)\n",
    "        actions = torch.Tensor(batch.action).to(device)\n",
    "        next_states = torch.stack(batch.next_state).squeeze(1)\n",
    "        rewards = torch.Tensor(batch.reward).to(device)\n",
    "        is_dones = torch.Tensor(batch.is_done).to(device)\n",
    "        \n",
    "        # This is the fundamental logic behind calulating a deep Q value.\n",
    "        curr_Q = self.model(curr_states).mul(actions).sum(1)\n",
    "        next_Q = self.model(next_states).max(1)[0]\n",
    "        expected_Q = rewards + (1 - is_dones) * self.gamma * next_Q\n",
    "\n",
    "        loss = self.loss_fn(expected_Q, curr_Q)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "    \n",
    "    def train(self, env, print_epochs=False):\n",
    "        for epoch in range(self.num_epochs):\n",
    "            is_done = False\n",
    "            curr_state = env.reset()\n",
    "            curr_state = torch.Tensor(curr_state).to(device)\n",
    "            score = 0\n",
    "            \n",
    "            while not is_done:\n",
    "\n",
    "                action = self.act(curr_state.unsqueeze(0))\n",
    "                next_state, reward, is_done, _ = env.step(action)\n",
    "                next_state = torch.Tensor(next_state).to(device)\n",
    "                action_encode = np.eye(2)[action].tolist()\n",
    "                self.memory.remember(curr_state, action_encode, next_state, reward, is_done)\n",
    "                      \n",
    "                self.train_step()\n",
    "\n",
    "                score += 1\n",
    "            \n",
    "            if print_epochs:\n",
    "                print(\"Epoch: \" + str(epoch + 1) + \". Score is: \" + str(score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1. Score is: 30\n",
      "Epoch: 2. Score is: 16\n",
      "Epoch: 3. Score is: 18\n",
      "Epoch: 4. Score is: 18\n",
      "Epoch: 5. Score is: 11\n",
      "Epoch: 6. Score is: 18\n",
      "Epoch: 7. Score is: 10\n",
      "Epoch: 8. Score is: 13\n",
      "Epoch: 9. Score is: 16\n",
      "Epoch: 10. Score is: 17\n",
      "Epoch: 11. Score is: 27\n",
      "Epoch: 12. Score is: 23\n",
      "Epoch: 13. Score is: 14\n",
      "Epoch: 14. Score is: 19\n",
      "Epoch: 15. Score is: 45\n",
      "Epoch: 16. Score is: 37\n",
      "Epoch: 17. Score is: 16\n",
      "Epoch: 18. Score is: 32\n",
      "Epoch: 19. Score is: 19\n",
      "Epoch: 20. Score is: 28\n",
      "Epoch: 21. Score is: 23\n",
      "Epoch: 22. Score is: 19\n",
      "Epoch: 23. Score is: 29\n",
      "Epoch: 24. Score is: 15\n",
      "Epoch: 25. Score is: 15\n",
      "Epoch: 26. Score is: 28\n",
      "Epoch: 27. Score is: 13\n",
      "Epoch: 28. Score is: 12\n",
      "Epoch: 29. Score is: 19\n",
      "Epoch: 30. Score is: 68\n",
      "Epoch: 31. Score is: 18\n",
      "Epoch: 32. Score is: 16\n",
      "Epoch: 33. Score is: 40\n",
      "Epoch: 34. Score is: 14\n",
      "Epoch: 35. Score is: 20\n",
      "Epoch: 36. Score is: 27\n",
      "Epoch: 37. Score is: 13\n",
      "Epoch: 38. Score is: 11\n",
      "Epoch: 39. Score is: 29\n",
      "Epoch: 40. Score is: 21\n",
      "Epoch: 41. Score is: 35\n",
      "Epoch: 42. Score is: 19\n",
      "Epoch: 43. Score is: 10\n",
      "Epoch: 44. Score is: 11\n",
      "Epoch: 45. Score is: 12\n",
      "Epoch: 46. Score is: 11\n",
      "Epoch: 47. Score is: 16\n",
      "Epoch: 48. Score is: 27\n",
      "Epoch: 49. Score is: 17\n",
      "Epoch: 50. Score is: 14\n",
      "Epoch: 51. Score is: 15\n",
      "Epoch: 52. Score is: 30\n",
      "Epoch: 53. Score is: 36\n",
      "Epoch: 54. Score is: 32\n",
      "Epoch: 55. Score is: 28\n",
      "Epoch: 56. Score is: 11\n",
      "Epoch: 57. Score is: 13\n",
      "Epoch: 58. Score is: 16\n",
      "Epoch: 59. Score is: 10\n",
      "Epoch: 60. Score is: 12\n",
      "Epoch: 61. Score is: 17\n",
      "Epoch: 62. Score is: 26\n",
      "Epoch: 63. Score is: 24\n",
      "Epoch: 64. Score is: 35\n",
      "Epoch: 65. Score is: 21\n",
      "Epoch: 66. Score is: 26\n",
      "Epoch: 67. Score is: 12\n",
      "Epoch: 68. Score is: 14\n",
      "Epoch: 69. Score is: 12\n",
      "Epoch: 70. Score is: 19\n",
      "Epoch: 71. Score is: 18\n",
      "Epoch: 72. Score is: 28\n",
      "Epoch: 73. Score is: 17\n",
      "Epoch: 74. Score is: 35\n",
      "Epoch: 75. Score is: 27\n",
      "Epoch: 76. Score is: 17\n",
      "Epoch: 77. Score is: 15\n",
      "Epoch: 78. Score is: 30\n",
      "Epoch: 79. Score is: 61\n",
      "Epoch: 80. Score is: 16\n",
      "Epoch: 81. Score is: 11\n",
      "Epoch: 82. Score is: 13\n",
      "Epoch: 83. Score is: 16\n",
      "Epoch: 84. Score is: 43\n",
      "Epoch: 85. Score is: 15\n",
      "Epoch: 86. Score is: 14\n",
      "Epoch: 87. Score is: 20\n",
      "Epoch: 88. Score is: 18\n",
      "Epoch: 89. Score is: 18\n",
      "Epoch: 90. Score is: 74\n",
      "Epoch: 91. Score is: 15\n",
      "Epoch: 92. Score is: 35\n",
      "Epoch: 93. Score is: 25\n",
      "Epoch: 94. Score is: 18\n",
      "Epoch: 95. Score is: 20\n",
      "Epoch: 96. Score is: 22\n",
      "Epoch: 97. Score is: 15\n",
      "Epoch: 98. Score is: 13\n",
      "Epoch: 99. Score is: 17\n",
      "Epoch: 100. Score is: 15\n",
      "Epoch: 101. Score is: 18\n",
      "Epoch: 102. Score is: 15\n",
      "Epoch: 103. Score is: 12\n",
      "Epoch: 104. Score is: 17\n",
      "Epoch: 105. Score is: 14\n",
      "Epoch: 106. Score is: 9\n",
      "Epoch: 107. Score is: 14\n",
      "Epoch: 108. Score is: 9\n",
      "Epoch: 109. Score is: 13\n",
      "Epoch: 110. Score is: 16\n",
      "Epoch: 111. Score is: 16\n",
      "Epoch: 112. Score is: 17\n",
      "Epoch: 113. Score is: 30\n",
      "Epoch: 114. Score is: 51\n",
      "Epoch: 115. Score is: 11\n",
      "Epoch: 116. Score is: 11\n",
      "Epoch: 117. Score is: 10\n",
      "Epoch: 118. Score is: 18\n",
      "Epoch: 119. Score is: 22\n",
      "Epoch: 120. Score is: 41\n",
      "Epoch: 121. Score is: 15\n",
      "Epoch: 122. Score is: 16\n",
      "Epoch: 123. Score is: 20\n",
      "Epoch: 124. Score is: 27\n",
      "Epoch: 125. Score is: 17\n",
      "Epoch: 126. Score is: 11\n",
      "Epoch: 127. Score is: 14\n",
      "Epoch: 128. Score is: 9\n",
      "Epoch: 129. Score is: 24\n",
      "Epoch: 130. Score is: 16\n",
      "Epoch: 131. Score is: 9\n",
      "Epoch: 132. Score is: 18\n",
      "Epoch: 133. Score is: 11\n",
      "Epoch: 134. Score is: 25\n",
      "Epoch: 135. Score is: 18\n",
      "Epoch: 136. Score is: 20\n",
      "Epoch: 137. Score is: 11\n",
      "Epoch: 138. Score is: 15\n",
      "Epoch: 139. Score is: 13\n",
      "Epoch: 140. Score is: 9\n",
      "Epoch: 141. Score is: 21\n",
      "Epoch: 142. Score is: 31\n",
      "Epoch: 143. Score is: 15\n",
      "Epoch: 144. Score is: 12\n",
      "Epoch: 145. Score is: 13\n",
      "Epoch: 146. Score is: 20\n",
      "Epoch: 147. Score is: 13\n",
      "Epoch: 148. Score is: 28\n",
      "Epoch: 149. Score is: 23\n",
      "Epoch: 150. Score is: 23\n",
      "Epoch: 151. Score is: 28\n",
      "Epoch: 152. Score is: 19\n",
      "Epoch: 153. Score is: 10\n",
      "Epoch: 154. Score is: 15\n",
      "Epoch: 155. Score is: 20\n",
      "Epoch: 156. Score is: 27\n",
      "Epoch: 157. Score is: 12\n",
      "Epoch: 158. Score is: 18\n",
      "Epoch: 159. Score is: 18\n",
      "Epoch: 160. Score is: 10\n",
      "Epoch: 161. Score is: 21\n",
      "Epoch: 162. Score is: 17\n",
      "Epoch: 163. Score is: 12\n",
      "Epoch: 164. Score is: 14\n",
      "Epoch: 165. Score is: 13\n",
      "Epoch: 166. Score is: 39\n",
      "Epoch: 167. Score is: 34\n",
      "Epoch: 168. Score is: 19\n",
      "Epoch: 169. Score is: 16\n",
      "Epoch: 170. Score is: 16\n",
      "Epoch: 171. Score is: 14\n",
      "Epoch: 172. Score is: 13\n",
      "Epoch: 173. Score is: 10\n",
      "Epoch: 174. Score is: 14\n",
      "Epoch: 175. Score is: 22\n",
      "Epoch: 176. Score is: 10\n",
      "Epoch: 177. Score is: 9\n",
      "Epoch: 178. Score is: 13\n",
      "Epoch: 179. Score is: 21\n",
      "Epoch: 180. Score is: 18\n",
      "Epoch: 181. Score is: 22\n",
      "Epoch: 182. Score is: 15\n",
      "Epoch: 183. Score is: 23\n",
      "Epoch: 184. Score is: 17\n",
      "Epoch: 185. Score is: 14\n",
      "Epoch: 186. Score is: 12\n",
      "Epoch: 187. Score is: 30\n",
      "Epoch: 188. Score is: 12\n",
      "Epoch: 189. Score is: 12\n",
      "Epoch: 190. Score is: 16\n",
      "Epoch: 191. Score is: 22\n",
      "Epoch: 192. Score is: 17\n",
      "Epoch: 193. Score is: 19\n",
      "Epoch: 194. Score is: 15\n",
      "Epoch: 195. Score is: 12\n",
      "Epoch: 196. Score is: 18\n",
      "Epoch: 197. Score is: 30\n",
      "Epoch: 198. Score is: 10\n",
      "Epoch: 199. Score is: 13\n",
      "Epoch: 200. Score is: 12\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "agent = DQNAgent(env)\n",
    "agent.train(env, print_epochs=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c0f8c7c35006bc217a7b718b84792749372e799ff4ec2664fb84cfa3b37974dd"
  },
  "kernelspec": {
   "display_name": "Python 3.8.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
