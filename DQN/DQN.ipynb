{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing dependancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition\n",
    "\n",
    "We can use the state variables (position of cart, angle of pole)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fn1 = nn.LazyLinear(64)\n",
    "        self.fn2 = nn.LazyLinear(64)\n",
    "        self.fn3 = nn.LazyLinear(output_size)\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.act(self.fn1(input))\n",
    "        x = self.act(self.fn2(x))\n",
    "        output = self.fn3(x)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory\n",
    "The agent should memorize what it learned before so it can learn from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = namedtuple('Experience', ('curr_state', 'action', 'next_state', 'reward', 'is_going'))\n",
    "\n",
    "class Memory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def remember(self, *args):\n",
    "        self.memory.append(Experience(*args))\n",
    "\n",
    "    def recall(self, batch_size):\n",
    "        experiences = random.sample(self.memory, batch_size)\n",
    "        batch = Experience(*zip(*experiences))\n",
    "        return batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, env):\n",
    "\n",
    "        self.action_size = env.action_space.n # output size\n",
    "        \n",
    "        self.model = DQN(self.action_size).to(device)\n",
    "\n",
    "        self.loss_fn = nn.SmoothL1Loss()\n",
    "        self.optimizer = optim.AdamW(self.model.parameters(), lr=0.0005)\n",
    "\n",
    "        self.exploration_rate = 1 # initial exploration rate, always leave at 1\n",
    "        self.exploration_rate_decay = 0.9999 # rate at which the exploration decreases\n",
    "        self.exploration_rate_min = 0.1 # minimun exploration rate\n",
    "        \n",
    "        self.gamma = 0.999 # falloff for Q score\n",
    "\n",
    "        self.batch_size = 64\n",
    "        self.num_epochs = 500\n",
    "\n",
    "        self.memory = Memory(10000) # how many of the previous samples are used\n",
    "    \n",
    "    def act(self, state):\n",
    "        if random.random() < self.exploration_rate:\n",
    "            action = random.randrange(self.action_size) # act randomly\n",
    "        else:\n",
    "            action = self.act_ideal(state) # act ideally\n",
    "            \n",
    "        self.exploration_rate *= self.exploration_rate_decay\n",
    "        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def act_ideal(self, state):\n",
    "        return self.model(state).max(1)[1].item()\n",
    "    \n",
    "    def train_step(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        batch = self.memory.recall(self.batch_size)\n",
    "        \n",
    "        curr_states = torch.stack(batch.curr_state).squeeze(1)\n",
    "        actions = torch.Tensor(batch.action).to(device)\n",
    "        next_states = torch.stack(batch.next_state).squeeze(1)\n",
    "        rewards = torch.Tensor(batch.reward).to(device)\n",
    "        is_goings = torch.Tensor(batch.is_going).to(device)\n",
    "        \n",
    "        # This is the fundamental logic behind calulating a deep Q value.\n",
    "        curr_Q = self.model(curr_states).mul(actions).sum(1)\n",
    "        next_Q = self.model(next_states).max(1)[0]\n",
    "        expected_Q = rewards + is_goings * self.gamma * next_Q\n",
    "\n",
    "        loss = self.loss_fn(expected_Q, curr_Q)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "    \n",
    "    def train(self, env, print_epochs=False):\n",
    "\n",
    "        scores = []\n",
    "\n",
    "        for epoch in range(self.num_epochs):\n",
    "            done = False\n",
    "            curr_state = env.reset()\n",
    "            curr_state = torch.Tensor(curr_state).to(device)\n",
    "            score = 0\n",
    "            \n",
    "            while not done:\n",
    "\n",
    "                action = self.act(curr_state.unsqueeze(0))\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                next_state = torch.Tensor(next_state).to(device)\n",
    "                action_encode = np.eye(2)[action]\n",
    "                self.memory.remember(curr_state, action_encode, next_state, reward, 1 - done)\n",
    "\n",
    "                curr_state = next_state\n",
    "                      \n",
    "                self.train_step()\n",
    "\n",
    "                score += 1\n",
    "            \n",
    "            scores.append(score)\n",
    "            if print_epochs:\n",
    "                print(\"Epoch: \" + str(epoch + 1) + \". Score is: \" + str(score))\n",
    "            \n",
    "        return scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sean Jan's PC\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1. Score is: 26\n",
      "Epoch: 2. Score is: 28\n",
      "Epoch: 3. Score is: 41\n",
      "Epoch: 4. Score is: 12\n",
      "Epoch: 5. Score is: 12\n",
      "Epoch: 6. Score is: 19\n",
      "Epoch: 7. Score is: 29\n",
      "Epoch: 8. Score is: 58\n",
      "Epoch: 9. Score is: 38\n",
      "Epoch: 10. Score is: 25\n",
      "Epoch: 11. Score is: 20\n",
      "Epoch: 12. Score is: 25\n",
      "Epoch: 13. Score is: 16\n",
      "Epoch: 14. Score is: 32\n",
      "Epoch: 15. Score is: 20\n",
      "Epoch: 16. Score is: 32\n",
      "Epoch: 17. Score is: 21\n",
      "Epoch: 18. Score is: 15\n",
      "Epoch: 19. Score is: 18\n",
      "Epoch: 20. Score is: 15\n",
      "Epoch: 21. Score is: 16\n",
      "Epoch: 22. Score is: 34\n",
      "Epoch: 23. Score is: 25\n",
      "Epoch: 24. Score is: 21\n",
      "Epoch: 25. Score is: 43\n",
      "Epoch: 26. Score is: 53\n",
      "Epoch: 27. Score is: 20\n",
      "Epoch: 28. Score is: 22\n",
      "Epoch: 29. Score is: 30\n",
      "Epoch: 30. Score is: 17\n",
      "Epoch: 31. Score is: 21\n",
      "Epoch: 32. Score is: 41\n",
      "Epoch: 33. Score is: 33\n",
      "Epoch: 34. Score is: 34\n",
      "Epoch: 35. Score is: 74\n",
      "Epoch: 36. Score is: 33\n",
      "Epoch: 37. Score is: 13\n",
      "Epoch: 38. Score is: 33\n",
      "Epoch: 39. Score is: 51\n",
      "Epoch: 40. Score is: 33\n",
      "Epoch: 41. Score is: 48\n",
      "Epoch: 42. Score is: 14\n",
      "Epoch: 43. Score is: 20\n",
      "Epoch: 44. Score is: 16\n",
      "Epoch: 45. Score is: 17\n",
      "Epoch: 46. Score is: 20\n",
      "Epoch: 47. Score is: 24\n",
      "Epoch: 48. Score is: 16\n",
      "Epoch: 49. Score is: 11\n",
      "Epoch: 50. Score is: 11\n",
      "Epoch: 51. Score is: 13\n",
      "Epoch: 52. Score is: 9\n",
      "Epoch: 53. Score is: 29\n",
      "Epoch: 54. Score is: 10\n",
      "Epoch: 55. Score is: 30\n",
      "Epoch: 56. Score is: 14\n",
      "Epoch: 57. Score is: 34\n",
      "Epoch: 58. Score is: 19\n",
      "Epoch: 59. Score is: 19\n",
      "Epoch: 60. Score is: 24\n",
      "Epoch: 61. Score is: 27\n",
      "Epoch: 62. Score is: 41\n",
      "Epoch: 63. Score is: 19\n",
      "Epoch: 64. Score is: 14\n",
      "Epoch: 65. Score is: 29\n",
      "Epoch: 66. Score is: 26\n",
      "Epoch: 67. Score is: 10\n",
      "Epoch: 68. Score is: 85\n",
      "Epoch: 69. Score is: 36\n",
      "Epoch: 70. Score is: 74\n",
      "Epoch: 71. Score is: 12\n",
      "Epoch: 72. Score is: 28\n",
      "Epoch: 73. Score is: 22\n",
      "Epoch: 74. Score is: 11\n",
      "Epoch: 75. Score is: 22\n",
      "Epoch: 76. Score is: 17\n",
      "Epoch: 77. Score is: 72\n",
      "Epoch: 78. Score is: 16\n",
      "Epoch: 79. Score is: 58\n",
      "Epoch: 80. Score is: 19\n",
      "Epoch: 81. Score is: 21\n",
      "Epoch: 82. Score is: 20\n",
      "Epoch: 83. Score is: 20\n",
      "Epoch: 84. Score is: 45\n",
      "Epoch: 85. Score is: 13\n",
      "Epoch: 86. Score is: 13\n",
      "Epoch: 87. Score is: 21\n",
      "Epoch: 88. Score is: 61\n",
      "Epoch: 89. Score is: 23\n",
      "Epoch: 90. Score is: 36\n",
      "Epoch: 91. Score is: 15\n",
      "Epoch: 92. Score is: 68\n",
      "Epoch: 93. Score is: 15\n",
      "Epoch: 94. Score is: 50\n",
      "Epoch: 95. Score is: 35\n",
      "Epoch: 96. Score is: 15\n",
      "Epoch: 97. Score is: 21\n",
      "Epoch: 98. Score is: 32\n",
      "Epoch: 99. Score is: 15\n",
      "Epoch: 100. Score is: 10\n",
      "Epoch: 101. Score is: 16\n",
      "Epoch: 102. Score is: 10\n",
      "Epoch: 103. Score is: 11\n",
      "Epoch: 104. Score is: 29\n",
      "Epoch: 105. Score is: 16\n",
      "Epoch: 106. Score is: 11\n",
      "Epoch: 107. Score is: 20\n",
      "Epoch: 108. Score is: 47\n",
      "Epoch: 109. Score is: 37\n",
      "Epoch: 110. Score is: 50\n",
      "Epoch: 111. Score is: 22\n",
      "Epoch: 112. Score is: 15\n",
      "Epoch: 113. Score is: 34\n",
      "Epoch: 114. Score is: 62\n",
      "Epoch: 115. Score is: 18\n",
      "Epoch: 116. Score is: 23\n",
      "Epoch: 117. Score is: 18\n",
      "Epoch: 118. Score is: 48\n",
      "Epoch: 119. Score is: 14\n",
      "Epoch: 120. Score is: 26\n",
      "Epoch: 121. Score is: 13\n",
      "Epoch: 122. Score is: 67\n",
      "Epoch: 123. Score is: 52\n",
      "Epoch: 124. Score is: 30\n",
      "Epoch: 125. Score is: 23\n",
      "Epoch: 126. Score is: 37\n",
      "Epoch: 127. Score is: 55\n",
      "Epoch: 128. Score is: 27\n",
      "Epoch: 129. Score is: 38\n",
      "Epoch: 130. Score is: 25\n",
      "Epoch: 131. Score is: 32\n",
      "Epoch: 132. Score is: 52\n",
      "Epoch: 133. Score is: 25\n",
      "Epoch: 134. Score is: 74\n",
      "Epoch: 135. Score is: 45\n",
      "Epoch: 136. Score is: 15\n",
      "Epoch: 137. Score is: 49\n",
      "Epoch: 138. Score is: 14\n",
      "Epoch: 139. Score is: 17\n",
      "Epoch: 140. Score is: 19\n",
      "Epoch: 141. Score is: 23\n",
      "Epoch: 142. Score is: 15\n",
      "Epoch: 143. Score is: 18\n",
      "Epoch: 144. Score is: 24\n",
      "Epoch: 145. Score is: 12\n",
      "Epoch: 146. Score is: 43\n",
      "Epoch: 147. Score is: 28\n",
      "Epoch: 148. Score is: 51\n",
      "Epoch: 149. Score is: 22\n",
      "Epoch: 150. Score is: 22\n",
      "Epoch: 151. Score is: 35\n",
      "Epoch: 152. Score is: 68\n",
      "Epoch: 153. Score is: 36\n",
      "Epoch: 154. Score is: 27\n",
      "Epoch: 155. Score is: 53\n",
      "Epoch: 156. Score is: 40\n",
      "Epoch: 157. Score is: 13\n",
      "Epoch: 158. Score is: 55\n",
      "Epoch: 159. Score is: 42\n",
      "Epoch: 160. Score is: 84\n",
      "Epoch: 161. Score is: 16\n",
      "Epoch: 162. Score is: 38\n",
      "Epoch: 163. Score is: 58\n",
      "Epoch: 164. Score is: 89\n",
      "Epoch: 165. Score is: 15\n",
      "Epoch: 166. Score is: 14\n",
      "Epoch: 167. Score is: 45\n",
      "Epoch: 168. Score is: 10\n",
      "Epoch: 169. Score is: 40\n",
      "Epoch: 170. Score is: 53\n",
      "Epoch: 171. Score is: 67\n",
      "Epoch: 172. Score is: 19\n",
      "Epoch: 173. Score is: 15\n",
      "Epoch: 174. Score is: 78\n",
      "Epoch: 175. Score is: 60\n",
      "Epoch: 176. Score is: 113\n",
      "Epoch: 177. Score is: 103\n",
      "Epoch: 178. Score is: 26\n",
      "Epoch: 179. Score is: 59\n",
      "Epoch: 180. Score is: 17\n",
      "Epoch: 181. Score is: 98\n",
      "Epoch: 182. Score is: 24\n",
      "Epoch: 183. Score is: 66\n",
      "Epoch: 184. Score is: 42\n",
      "Epoch: 185. Score is: 66\n",
      "Epoch: 186. Score is: 43\n",
      "Epoch: 187. Score is: 64\n",
      "Epoch: 188. Score is: 177\n",
      "Epoch: 189. Score is: 64\n",
      "Epoch: 190. Score is: 48\n",
      "Epoch: 191. Score is: 68\n",
      "Epoch: 192. Score is: 142\n",
      "Epoch: 193. Score is: 64\n",
      "Epoch: 194. Score is: 75\n",
      "Epoch: 195. Score is: 24\n",
      "Epoch: 196. Score is: 76\n",
      "Epoch: 197. Score is: 16\n",
      "Epoch: 198. Score is: 99\n",
      "Epoch: 199. Score is: 25\n",
      "Epoch: 200. Score is: 22\n",
      "Epoch: 201. Score is: 128\n",
      "Epoch: 202. Score is: 51\n",
      "Epoch: 203. Score is: 26\n",
      "Epoch: 204. Score is: 64\n",
      "Epoch: 205. Score is: 56\n",
      "Epoch: 206. Score is: 82\n",
      "Epoch: 207. Score is: 55\n",
      "Epoch: 208. Score is: 32\n",
      "Epoch: 209. Score is: 155\n",
      "Epoch: 210. Score is: 109\n",
      "Epoch: 211. Score is: 79\n",
      "Epoch: 212. Score is: 64\n",
      "Epoch: 213. Score is: 54\n",
      "Epoch: 214. Score is: 87\n",
      "Epoch: 215. Score is: 15\n",
      "Epoch: 216. Score is: 69\n",
      "Epoch: 217. Score is: 23\n",
      "Epoch: 218. Score is: 169\n",
      "Epoch: 219. Score is: 42\n",
      "Epoch: 220. Score is: 41\n",
      "Epoch: 221. Score is: 83\n",
      "Epoch: 222. Score is: 80\n",
      "Epoch: 223. Score is: 48\n",
      "Epoch: 224. Score is: 73\n",
      "Epoch: 225. Score is: 29\n",
      "Epoch: 226. Score is: 81\n",
      "Epoch: 227. Score is: 13\n",
      "Epoch: 228. Score is: 77\n",
      "Epoch: 229. Score is: 46\n",
      "Epoch: 230. Score is: 53\n",
      "Epoch: 231. Score is: 88\n",
      "Epoch: 232. Score is: 90\n",
      "Epoch: 233. Score is: 123\n",
      "Epoch: 234. Score is: 64\n",
      "Epoch: 235. Score is: 108\n",
      "Epoch: 236. Score is: 126\n",
      "Epoch: 237. Score is: 84\n",
      "Epoch: 238. Score is: 146\n",
      "Epoch: 239. Score is: 200\n",
      "Epoch: 240. Score is: 53\n",
      "Epoch: 241. Score is: 71\n",
      "Epoch: 242. Score is: 75\n",
      "Epoch: 243. Score is: 93\n",
      "Epoch: 244. Score is: 163\n",
      "Epoch: 245. Score is: 16\n",
      "Epoch: 246. Score is: 79\n",
      "Epoch: 247. Score is: 121\n",
      "Epoch: 248. Score is: 142\n",
      "Epoch: 249. Score is: 92\n",
      "Epoch: 250. Score is: 67\n",
      "Epoch: 251. Score is: 50\n",
      "Epoch: 252. Score is: 168\n",
      "Epoch: 253. Score is: 70\n",
      "Epoch: 254. Score is: 75\n",
      "Epoch: 255. Score is: 54\n",
      "Epoch: 256. Score is: 99\n",
      "Epoch: 257. Score is: 22\n",
      "Epoch: 258. Score is: 65\n",
      "Epoch: 259. Score is: 52\n",
      "Epoch: 260. Score is: 95\n",
      "Epoch: 261. Score is: 62\n",
      "Epoch: 262. Score is: 56\n",
      "Epoch: 263. Score is: 87\n",
      "Epoch: 264. Score is: 77\n",
      "Epoch: 265. Score is: 79\n",
      "Epoch: 266. Score is: 53\n",
      "Epoch: 267. Score is: 52\n",
      "Epoch: 268. Score is: 138\n",
      "Epoch: 269. Score is: 129\n",
      "Epoch: 270. Score is: 112\n",
      "Epoch: 271. Score is: 51\n",
      "Epoch: 272. Score is: 65\n",
      "Epoch: 273. Score is: 90\n",
      "Epoch: 274. Score is: 78\n",
      "Epoch: 275. Score is: 200\n",
      "Epoch: 276. Score is: 61\n",
      "Epoch: 277. Score is: 163\n",
      "Epoch: 278. Score is: 82\n",
      "Epoch: 279. Score is: 100\n",
      "Epoch: 280. Score is: 173\n",
      "Epoch: 281. Score is: 65\n",
      "Epoch: 282. Score is: 105\n",
      "Epoch: 283. Score is: 22\n",
      "Epoch: 284. Score is: 55\n",
      "Epoch: 285. Score is: 66\n",
      "Epoch: 286. Score is: 101\n",
      "Epoch: 287. Score is: 85\n",
      "Epoch: 288. Score is: 137\n",
      "Epoch: 289. Score is: 129\n",
      "Epoch: 290. Score is: 107\n",
      "Epoch: 291. Score is: 200\n",
      "Epoch: 292. Score is: 129\n",
      "Epoch: 293. Score is: 51\n",
      "Epoch: 294. Score is: 78\n",
      "Epoch: 295. Score is: 110\n",
      "Epoch: 296. Score is: 24\n",
      "Epoch: 297. Score is: 139\n",
      "Epoch: 298. Score is: 41\n",
      "Epoch: 299. Score is: 71\n",
      "Epoch: 300. Score is: 106\n",
      "Epoch: 301. Score is: 143\n",
      "Epoch: 302. Score is: 56\n",
      "Epoch: 303. Score is: 191\n",
      "Epoch: 304. Score is: 200\n",
      "Epoch: 305. Score is: 46\n",
      "Epoch: 306. Score is: 172\n",
      "Epoch: 307. Score is: 66\n",
      "Epoch: 308. Score is: 189\n",
      "Epoch: 309. Score is: 88\n",
      "Epoch: 310. Score is: 106\n",
      "Epoch: 311. Score is: 158\n",
      "Epoch: 312. Score is: 137\n",
      "Epoch: 313. Score is: 138\n",
      "Epoch: 314. Score is: 114\n",
      "Epoch: 315. Score is: 161\n",
      "Epoch: 316. Score is: 200\n",
      "Epoch: 317. Score is: 200\n",
      "Epoch: 318. Score is: 86\n",
      "Epoch: 319. Score is: 109\n",
      "Epoch: 320. Score is: 200\n",
      "Epoch: 321. Score is: 177\n",
      "Epoch: 322. Score is: 48\n",
      "Epoch: 323. Score is: 87\n",
      "Epoch: 324. Score is: 139\n",
      "Epoch: 325. Score is: 196\n",
      "Epoch: 326. Score is: 56\n",
      "Epoch: 327. Score is: 191\n",
      "Epoch: 328. Score is: 181\n",
      "Epoch: 329. Score is: 103\n",
      "Epoch: 330. Score is: 92\n",
      "Epoch: 331. Score is: 111\n",
      "Epoch: 332. Score is: 24\n",
      "Epoch: 333. Score is: 144\n",
      "Epoch: 334. Score is: 45\n",
      "Epoch: 335. Score is: 180\n",
      "Epoch: 336. Score is: 104\n",
      "Epoch: 337. Score is: 161\n",
      "Epoch: 338. Score is: 175\n",
      "Epoch: 339. Score is: 86\n",
      "Epoch: 340. Score is: 84\n",
      "Epoch: 341. Score is: 186\n",
      "Epoch: 342. Score is: 198\n",
      "Epoch: 343. Score is: 174\n",
      "Epoch: 344. Score is: 83\n",
      "Epoch: 345. Score is: 142\n",
      "Epoch: 346. Score is: 176\n",
      "Epoch: 347. Score is: 135\n",
      "Epoch: 348. Score is: 125\n",
      "Epoch: 349. Score is: 175\n",
      "Epoch: 350. Score is: 75\n",
      "Epoch: 351. Score is: 192\n",
      "Epoch: 352. Score is: 148\n",
      "Epoch: 353. Score is: 178\n",
      "Epoch: 354. Score is: 64\n",
      "Epoch: 355. Score is: 200\n",
      "Epoch: 356. Score is: 200\n",
      "Epoch: 357. Score is: 148\n",
      "Epoch: 358. Score is: 200\n",
      "Epoch: 359. Score is: 200\n",
      "Epoch: 360. Score is: 200\n",
      "Epoch: 361. Score is: 200\n",
      "Epoch: 362. Score is: 200\n",
      "Epoch: 363. Score is: 93\n",
      "Epoch: 364. Score is: 118\n",
      "Epoch: 365. Score is: 200\n",
      "Epoch: 366. Score is: 123\n",
      "Epoch: 367. Score is: 200\n",
      "Epoch: 368. Score is: 200\n",
      "Epoch: 369. Score is: 82\n",
      "Epoch: 370. Score is: 148\n",
      "Epoch: 371. Score is: 46\n",
      "Epoch: 372. Score is: 105\n",
      "Epoch: 373. Score is: 200\n",
      "Epoch: 374. Score is: 200\n",
      "Epoch: 375. Score is: 157\n",
      "Epoch: 376. Score is: 73\n",
      "Epoch: 377. Score is: 200\n",
      "Epoch: 378. Score is: 200\n",
      "Epoch: 379. Score is: 146\n",
      "Epoch: 380. Score is: 200\n",
      "Epoch: 381. Score is: 148\n",
      "Epoch: 382. Score is: 200\n",
      "Epoch: 383. Score is: 200\n",
      "Epoch: 384. Score is: 200\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24140\\736844392.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0magent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDQNAgent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24140\\3288238119.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, env, print_epochs)\u001b[0m\n\u001b[0;32m     75\u001b[0m                 \u001b[0mcurr_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m                 \u001b[0mscore\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24140\\3288238119.py\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0mactions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0mnext_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[0mrewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[0mis_goings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_going\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "agent = DQNAgent(env)\n",
    "env.reset()\n",
    "scores = agent.train(env, print_epochs=True)\n",
    "plt.plot(scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115.5\n"
     ]
    }
   ],
   "source": [
    "test_scores = []\n",
    "\n",
    "for i in range(100):\n",
    "\n",
    "    done = False\n",
    "    curr_state = env.reset()\n",
    "    curr_state = torch.Tensor(curr_state).to(device)\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "\n",
    "        action = agent.act_ideal(curr_state.unsqueeze(0))\n",
    "        curr_state, _, done, _ = env.step(action)\n",
    "        curr_state = torch.Tensor(curr_state).to(device)\n",
    "\n",
    "        score += 1\n",
    "    \n",
    "    test_scores.append(score)\n",
    "    \n",
    "avg = sum(test_scores) / 100\n",
    "print(avg)\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c0f8c7c35006bc217a7b718b84792749372e799ff4ec2664fb84cfa3b37974dd"
  },
  "kernelspec": {
   "display_name": "Python 3.8.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
