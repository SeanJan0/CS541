{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing dependancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition\n",
    "\n",
    "Using image as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_shape, output_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding='same')\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, padding='same')\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, padding='same')\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        self.flatten = nn.Flatten()\n",
    "        linear_input = input_shape[1] * input_shape[2] * 32\n",
    "        self.fn1 = nn.Linear(linear_input, 128)\n",
    "        self.fn2 = nn.Linear(128, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fn1(x))\n",
    "        x = F.relu(self.fn2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input extraction\n",
    "\n",
    "Taking the screen and turning it into something the model can understand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_screen(env, resize_shape):\n",
    "    screen = env.render(mode='rgb_array').transpose((2, 0, 1))\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    screen = torch.from_numpy(screen)\n",
    "    resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(resize_shape),\n",
    "                    T.ToTensor()])\n",
    "    return resize(screen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory\n",
    "The agent should memorize what it learned before so it can learn from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = namedtuple('Experience', ('curr_screen', 'action', 'next_screen', 'reward', 'still_going'))\n",
    "\n",
    "class Memory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def remember(self, *args):\n",
    "        self.memory.append(Experience(*args))\n",
    "\n",
    "    def recall(self, batch_size):\n",
    "        experiences = random.sample(self.memory, batch_size)\n",
    "        batch = Experience(*zip(*experiences))\n",
    "        return batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, env):\n",
    "        self.img_size = 40\n",
    "\n",
    "        env.reset()\n",
    "        sample_img = get_screen(env, self.img_size)\n",
    "\n",
    "        self.state_shape = sample_img.numpy().shape\n",
    "        self.action_size = env.action_space.n # output\n",
    "\n",
    "        self.model = DQN(self.state_shape, self.action_size).to(device)\n",
    "        \n",
    "        self.loss_fn = nn.SmoothL1Loss()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
    "\n",
    "        self.exploration_rate = 1 # initial exploration rate, always leave at 1\n",
    "        self.exploration_rate_decay = 0.9999 # rate at which the exploration decreases\n",
    "        self.exploration_rate_min = 0.1 # minimun exploration rate\n",
    "        \n",
    "        self.gamma = 0.99 # falloff for Q score\n",
    "\n",
    "        self.batch_size = 64\n",
    "        self.num_epochs = 500\n",
    "\n",
    "        self.memory = Memory(10000) # how many of the previous samples are used\n",
    "    \n",
    "    def act(self, screen):\n",
    "        if random.random() < self.exploration_rate:\n",
    "            action = random.randrange(self.action_size) # act randomly\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                action = self.act_ideal(screen) # act ideally\n",
    "            \n",
    "        self.exploration_rate *= self.exploration_rate_decay\n",
    "        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def act_ideal(self, screen):\n",
    "        return self.model(screen).max(1)[1].item()\n",
    "    \n",
    "    def train_step(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        batch = self.memory.recall(self.batch_size)\n",
    "        \n",
    "        curr_screens = torch.stack(batch.curr_screen).squeeze(1)\n",
    "        actions = torch.Tensor(batch.action).to(device)\n",
    "        next_screens = torch.stack(batch.next_screen).squeeze(1)\n",
    "        rewards = torch.Tensor(batch.reward).to(device)\n",
    "        still_goings = torch.Tensor(batch.still_going).to(device)\n",
    "        \n",
    "        # This is the fundamental logic behind calulating a deep Q value.\n",
    "        curr_Q = self.model(curr_screens).mul(actions).sum(1)\n",
    "        next_Q = self.model(next_screens).max(1)[0]\n",
    "        expected_Q = rewards + still_goings * self.gamma * next_Q\n",
    "\n",
    "        loss = self.loss_fn(expected_Q, curr_Q)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "    \n",
    "    def train(self, env, print_epochs=False):\n",
    "        for epoch in range(self.num_epochs):\n",
    "            done = False\n",
    "            env.reset()\n",
    "            score = 0\n",
    "            \n",
    "            while not done:\n",
    "                curr_screen = get_screen(env, self.img_size)\n",
    "\n",
    "                action = self.act(curr_screen.unsqueeze(0))\n",
    "                _, reward, done, _ = env.step(action)\n",
    "                next_screen = get_screen(env, self.img_size)\n",
    "                action_encode = np.eye(2)[action].tolist()\n",
    "                self.memory.remember(curr_screen, action_encode, next_screen, reward, 1 - done)\n",
    "                      \n",
    "                self.train_step()\n",
    "\n",
    "                score += 1\n",
    "            \n",
    "            if print_epochs:\n",
    "                print(\"Epoch: \" + str(epoch + 1) + \". Score is: \" + str(score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1. Score is: 19\n",
      "Epoch: 2. Score is: 15\n",
      "Epoch: 3. Score is: 13\n",
      "Epoch: 4. Score is: 42\n",
      "Epoch: 5. Score is: 35\n",
      "Epoch: 6. Score is: 13\n",
      "Epoch: 7. Score is: 17\n",
      "Epoch: 8. Score is: 23\n",
      "Epoch: 9. Score is: 14\n",
      "Epoch: 10. Score is: 40\n",
      "Epoch: 11. Score is: 9\n",
      "Epoch: 12. Score is: 31\n",
      "Epoch: 13. Score is: 24\n",
      "Epoch: 14. Score is: 14\n",
      "Epoch: 15. Score is: 30\n",
      "Epoch: 16. Score is: 55\n",
      "Epoch: 17. Score is: 34\n",
      "Epoch: 18. Score is: 30\n",
      "Epoch: 19. Score is: 16\n",
      "Epoch: 20. Score is: 29\n",
      "Epoch: 21. Score is: 33\n",
      "Epoch: 22. Score is: 14\n",
      "Epoch: 23. Score is: 22\n",
      "Epoch: 24. Score is: 33\n",
      "Epoch: 25. Score is: 14\n",
      "Epoch: 26. Score is: 15\n",
      "Epoch: 27. Score is: 17\n",
      "Epoch: 28. Score is: 59\n",
      "Epoch: 29. Score is: 16\n",
      "Epoch: 30. Score is: 22\n",
      "Epoch: 31. Score is: 32\n",
      "Epoch: 32. Score is: 25\n",
      "Epoch: 33. Score is: 12\n",
      "Epoch: 34. Score is: 20\n",
      "Epoch: 35. Score is: 14\n",
      "Epoch: 36. Score is: 32\n",
      "Epoch: 37. Score is: 14\n",
      "Epoch: 38. Score is: 36\n",
      "Epoch: 39. Score is: 17\n",
      "Epoch: 40. Score is: 21\n",
      "Epoch: 41. Score is: 16\n",
      "Epoch: 42. Score is: 66\n",
      "Epoch: 43. Score is: 10\n",
      "Epoch: 44. Score is: 14\n",
      "Epoch: 45. Score is: 15\n",
      "Epoch: 46. Score is: 50\n",
      "Epoch: 47. Score is: 43\n",
      "Epoch: 48. Score is: 15\n",
      "Epoch: 49. Score is: 29\n",
      "Epoch: 50. Score is: 16\n",
      "Epoch: 51. Score is: 32\n",
      "Epoch: 52. Score is: 29\n",
      "Epoch: 53. Score is: 10\n",
      "Epoch: 54. Score is: 43\n",
      "Epoch: 55. Score is: 22\n",
      "Epoch: 56. Score is: 27\n",
      "Epoch: 57. Score is: 17\n",
      "Epoch: 58. Score is: 23\n",
      "Epoch: 59. Score is: 12\n",
      "Epoch: 60. Score is: 26\n",
      "Epoch: 61. Score is: 19\n",
      "Epoch: 62. Score is: 34\n",
      "Epoch: 63. Score is: 14\n",
      "Epoch: 64. Score is: 20\n",
      "Epoch: 65. Score is: 24\n",
      "Epoch: 66. Score is: 28\n",
      "Epoch: 67. Score is: 25\n",
      "Epoch: 68. Score is: 20\n",
      "Epoch: 69. Score is: 28\n",
      "Epoch: 70. Score is: 16\n",
      "Epoch: 71. Score is: 32\n",
      "Epoch: 72. Score is: 16\n",
      "Epoch: 73. Score is: 14\n",
      "Epoch: 74. Score is: 22\n",
      "Epoch: 75. Score is: 42\n",
      "Epoch: 76. Score is: 21\n",
      "Epoch: 77. Score is: 23\n",
      "Epoch: 78. Score is: 21\n",
      "Epoch: 79. Score is: 13\n",
      "Epoch: 80. Score is: 40\n",
      "Epoch: 81. Score is: 15\n",
      "Epoch: 82. Score is: 17\n",
      "Epoch: 83. Score is: 27\n",
      "Epoch: 84. Score is: 15\n",
      "Epoch: 85. Score is: 19\n",
      "Epoch: 86. Score is: 27\n",
      "Epoch: 87. Score is: 10\n",
      "Epoch: 88. Score is: 13\n",
      "Epoch: 89. Score is: 25\n",
      "Epoch: 90. Score is: 12\n",
      "Epoch: 91. Score is: 16\n",
      "Epoch: 92. Score is: 13\n",
      "Epoch: 93. Score is: 16\n",
      "Epoch: 94. Score is: 26\n",
      "Epoch: 95. Score is: 16\n",
      "Epoch: 96. Score is: 10\n",
      "Epoch: 97. Score is: 13\n",
      "Epoch: 98. Score is: 21\n",
      "Epoch: 99. Score is: 16\n",
      "Epoch: 100. Score is: 20\n",
      "Epoch: 101. Score is: 13\n",
      "Epoch: 102. Score is: 42\n",
      "Epoch: 103. Score is: 13\n",
      "Epoch: 104. Score is: 36\n",
      "Epoch: 105. Score is: 27\n",
      "Epoch: 106. Score is: 11\n",
      "Epoch: 107. Score is: 25\n",
      "Epoch: 108. Score is: 17\n",
      "Epoch: 109. Score is: 16\n",
      "Epoch: 110. Score is: 30\n",
      "Epoch: 111. Score is: 10\n",
      "Epoch: 112. Score is: 17\n",
      "Epoch: 113. Score is: 17\n",
      "Epoch: 114. Score is: 15\n",
      "Epoch: 115. Score is: 23\n",
      "Epoch: 116. Score is: 15\n",
      "Epoch: 117. Score is: 11\n",
      "Epoch: 118. Score is: 12\n",
      "Epoch: 119. Score is: 42\n",
      "Epoch: 120. Score is: 19\n",
      "Epoch: 121. Score is: 53\n",
      "Epoch: 122. Score is: 18\n",
      "Epoch: 123. Score is: 38\n",
      "Epoch: 124. Score is: 11\n",
      "Epoch: 125. Score is: 31\n",
      "Epoch: 126. Score is: 12\n",
      "Epoch: 127. Score is: 11\n",
      "Epoch: 128. Score is: 10\n",
      "Epoch: 129. Score is: 31\n",
      "Epoch: 130. Score is: 19\n",
      "Epoch: 131. Score is: 11\n",
      "Epoch: 132. Score is: 30\n",
      "Epoch: 133. Score is: 13\n",
      "Epoch: 134. Score is: 11\n",
      "Epoch: 135. Score is: 23\n",
      "Epoch: 136. Score is: 15\n",
      "Epoch: 137. Score is: 27\n",
      "Epoch: 138. Score is: 11\n",
      "Epoch: 139. Score is: 19\n",
      "Epoch: 140. Score is: 16\n",
      "Epoch: 141. Score is: 51\n",
      "Epoch: 142. Score is: 16\n",
      "Epoch: 143. Score is: 14\n",
      "Epoch: 144. Score is: 15\n",
      "Epoch: 145. Score is: 32\n",
      "Epoch: 146. Score is: 24\n",
      "Epoch: 147. Score is: 23\n",
      "Epoch: 148. Score is: 12\n",
      "Epoch: 149. Score is: 16\n",
      "Epoch: 150. Score is: 15\n",
      "Epoch: 151. Score is: 42\n",
      "Epoch: 152. Score is: 10\n",
      "Epoch: 153. Score is: 25\n",
      "Epoch: 154. Score is: 24\n",
      "Epoch: 155. Score is: 13\n",
      "Epoch: 156. Score is: 28\n",
      "Epoch: 157. Score is: 18\n",
      "Epoch: 158. Score is: 20\n",
      "Epoch: 159. Score is: 18\n",
      "Epoch: 160. Score is: 18\n",
      "Epoch: 161. Score is: 19\n",
      "Epoch: 162. Score is: 14\n",
      "Epoch: 163. Score is: 12\n",
      "Epoch: 164. Score is: 28\n",
      "Epoch: 165. Score is: 15\n",
      "Epoch: 166. Score is: 26\n",
      "Epoch: 167. Score is: 15\n",
      "Epoch: 168. Score is: 26\n",
      "Epoch: 169. Score is: 26\n",
      "Epoch: 170. Score is: 58\n",
      "Epoch: 171. Score is: 12\n",
      "Epoch: 172. Score is: 12\n",
      "Epoch: 173. Score is: 21\n",
      "Epoch: 174. Score is: 12\n",
      "Epoch: 175. Score is: 20\n",
      "Epoch: 176. Score is: 10\n",
      "Epoch: 177. Score is: 19\n",
      "Epoch: 178. Score is: 41\n",
      "Epoch: 179. Score is: 15\n",
      "Epoch: 180. Score is: 17\n",
      "Epoch: 181. Score is: 38\n",
      "Epoch: 182. Score is: 13\n",
      "Epoch: 183. Score is: 21\n",
      "Epoch: 184. Score is: 29\n",
      "Epoch: 185. Score is: 11\n",
      "Epoch: 186. Score is: 24\n",
      "Epoch: 187. Score is: 25\n",
      "Epoch: 188. Score is: 12\n",
      "Epoch: 189. Score is: 21\n",
      "Epoch: 190. Score is: 29\n",
      "Epoch: 191. Score is: 19\n",
      "Epoch: 192. Score is: 11\n",
      "Epoch: 193. Score is: 15\n",
      "Epoch: 194. Score is: 22\n",
      "Epoch: 195. Score is: 13\n",
      "Epoch: 196. Score is: 17\n",
      "Epoch: 197. Score is: 16\n",
      "Epoch: 198. Score is: 28\n",
      "Epoch: 199. Score is: 19\n",
      "Epoch: 200. Score is: 50\n",
      "Epoch: 201. Score is: 73\n",
      "Epoch: 202. Score is: 19\n",
      "Epoch: 203. Score is: 39\n",
      "Epoch: 204. Score is: 28\n",
      "Epoch: 205. Score is: 19\n",
      "Epoch: 206. Score is: 18\n",
      "Epoch: 207. Score is: 23\n",
      "Epoch: 208. Score is: 19\n",
      "Epoch: 209. Score is: 14\n",
      "Epoch: 210. Score is: 13\n",
      "Epoch: 211. Score is: 51\n",
      "Epoch: 212. Score is: 20\n",
      "Epoch: 213. Score is: 13\n",
      "Epoch: 214. Score is: 9\n",
      "Epoch: 215. Score is: 21\n",
      "Epoch: 216. Score is: 28\n",
      "Epoch: 217. Score is: 15\n",
      "Epoch: 218. Score is: 17\n",
      "Epoch: 219. Score is: 14\n",
      "Epoch: 220. Score is: 35\n",
      "Epoch: 221. Score is: 10\n",
      "Epoch: 222. Score is: 25\n",
      "Epoch: 223. Score is: 13\n",
      "Epoch: 224. Score is: 11\n",
      "Epoch: 225. Score is: 25\n",
      "Epoch: 226. Score is: 11\n",
      "Epoch: 227. Score is: 25\n",
      "Epoch: 228. Score is: 22\n",
      "Epoch: 229. Score is: 18\n",
      "Epoch: 230. Score is: 18\n",
      "Epoch: 231. Score is: 25\n",
      "Epoch: 232. Score is: 12\n",
      "Epoch: 233. Score is: 17\n",
      "Epoch: 234. Score is: 20\n",
      "Epoch: 235. Score is: 14\n",
      "Epoch: 236. Score is: 27\n",
      "Epoch: 237. Score is: 10\n",
      "Epoch: 238. Score is: 10\n",
      "Epoch: 239. Score is: 13\n",
      "Epoch: 240. Score is: 19\n",
      "Epoch: 241. Score is: 29\n",
      "Epoch: 242. Score is: 13\n",
      "Epoch: 243. Score is: 12\n",
      "Epoch: 244. Score is: 13\n",
      "Epoch: 245. Score is: 23\n",
      "Epoch: 246. Score is: 29\n",
      "Epoch: 247. Score is: 13\n",
      "Epoch: 248. Score is: 18\n",
      "Epoch: 249. Score is: 27\n",
      "Epoch: 250. Score is: 38\n",
      "Epoch: 251. Score is: 15\n",
      "Epoch: 252. Score is: 23\n",
      "Epoch: 253. Score is: 17\n",
      "Epoch: 254. Score is: 19\n",
      "Epoch: 255. Score is: 15\n",
      "Epoch: 256. Score is: 54\n",
      "Epoch: 257. Score is: 18\n",
      "Epoch: 258. Score is: 30\n",
      "Epoch: 259. Score is: 21\n",
      "Epoch: 260. Score is: 28\n",
      "Epoch: 261. Score is: 33\n",
      "Epoch: 262. Score is: 25\n",
      "Epoch: 263. Score is: 17\n",
      "Epoch: 264. Score is: 31\n",
      "Epoch: 265. Score is: 14\n",
      "Epoch: 266. Score is: 19\n",
      "Epoch: 267. Score is: 15\n",
      "Epoch: 268. Score is: 12\n",
      "Epoch: 269. Score is: 41\n",
      "Epoch: 270. Score is: 14\n",
      "Epoch: 271. Score is: 40\n",
      "Epoch: 272. Score is: 31\n",
      "Epoch: 273. Score is: 15\n",
      "Epoch: 274. Score is: 31\n",
      "Epoch: 275. Score is: 30\n",
      "Epoch: 276. Score is: 22\n",
      "Epoch: 277. Score is: 41\n",
      "Epoch: 278. Score is: 16\n",
      "Epoch: 279. Score is: 39\n",
      "Epoch: 280. Score is: 9\n",
      "Epoch: 281. Score is: 13\n",
      "Epoch: 282. Score is: 14\n",
      "Epoch: 283. Score is: 13\n",
      "Epoch: 284. Score is: 12\n",
      "Epoch: 285. Score is: 38\n",
      "Epoch: 286. Score is: 12\n",
      "Epoch: 287. Score is: 16\n",
      "Epoch: 288. Score is: 10\n",
      "Epoch: 289. Score is: 39\n",
      "Epoch: 290. Score is: 13\n",
      "Epoch: 291. Score is: 35\n",
      "Epoch: 292. Score is: 20\n",
      "Epoch: 293. Score is: 47\n",
      "Epoch: 294. Score is: 9\n",
      "Epoch: 295. Score is: 40\n",
      "Epoch: 296. Score is: 16\n",
      "Epoch: 297. Score is: 21\n",
      "Epoch: 298. Score is: 19\n",
      "Epoch: 299. Score is: 20\n",
      "Epoch: 300. Score is: 15\n",
      "Epoch: 301. Score is: 19\n",
      "Epoch: 302. Score is: 12\n",
      "Epoch: 303. Score is: 15\n",
      "Epoch: 304. Score is: 21\n",
      "Epoch: 305. Score is: 14\n",
      "Epoch: 306. Score is: 24\n",
      "Epoch: 307. Score is: 24\n",
      "Epoch: 308. Score is: 22\n",
      "Epoch: 309. Score is: 23\n",
      "Epoch: 310. Score is: 14\n",
      "Epoch: 311. Score is: 11\n",
      "Epoch: 312. Score is: 63\n",
      "Epoch: 313. Score is: 15\n",
      "Epoch: 314. Score is: 42\n",
      "Epoch: 315. Score is: 37\n",
      "Epoch: 316. Score is: 27\n",
      "Epoch: 317. Score is: 24\n",
      "Epoch: 318. Score is: 21\n",
      "Epoch: 319. Score is: 24\n",
      "Epoch: 320. Score is: 18\n",
      "Epoch: 321. Score is: 18\n",
      "Epoch: 322. Score is: 24\n",
      "Epoch: 323. Score is: 43\n",
      "Epoch: 324. Score is: 12\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "agent = DQNAgent(env)\n",
    "agent.train(env, print_epochs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.37\n"
     ]
    }
   ],
   "source": [
    "test_scores = []\n",
    "\n",
    "for i in range(100):\n",
    "    done = False\n",
    "    env.reset()\n",
    "    count = 0\n",
    "    \n",
    "    while not done:\n",
    "        curr_screen = get_screen(env, agent.img_size)\n",
    "        action = agent.act_ideal(curr_screen.unsqueeze(0))\n",
    "        _, _, done, _ = env.step(action)\n",
    "        count += 1\n",
    "        \n",
    "    test_scores.append(count)\n",
    "    \n",
    "avg = sum(test_scores) / 100\n",
    "print(avg)\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c0f8c7c35006bc217a7b718b84792749372e799ff4ec2664fb84cfa3b37974dd"
  },
  "kernelspec": {
   "display_name": "Python 3.8.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
